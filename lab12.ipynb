{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: 12\n",
    "## Write a Python program to implementation the Backpropagation Algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(input_size, hidden_layer_sizes, output_size, num_iterations, X, Y,\n",
    "learning_rate):\n",
    "    layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "    weights = [np.random.randn(layer_sizes[i + 1], layer_sizes[i]) for i in\n",
    "    range(len(layer_sizes) - 1)]\n",
    "    biases = [np.zeros((layer_sizes[i + 1], 1)) for i in range(len(layer_sizes) - 1)]\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "    for j in range(len(weights)):\n",
    "        z = np.dot(weights[j], activations[-1]) + biases[j]\n",
    "        zs.append(z)\n",
    "        a = sigmoid(z)\n",
    "        activations.append(a)\n",
    "        error = activations[-1] - Y\n",
    "        cost = (1 / (2 * X.shape[1])) * np.sum(np.square(error))\n",
    "        costs.append(cost)\n",
    "        dZ = [activations[-1] - Y]\n",
    "        dW = []\n",
    "        db = []\n",
    "    for j in range(len(weights) - 1, -1, -1):\n",
    "        dW.insert(0, (1 / X.shape[1]) * np.dot(dZ[0], activations[j - 1].T))\n",
    "        db.insert(0, (1 / X.shape[1]) * np.sum(dZ[0], axis=1, keepdims=True))\n",
    "        if j > 0:\n",
    "            dZ.insert(0, np.dot(weights[j].T, dZ[0]) * sigmoid_derivative(activations[j - 1]))\n",
    "    for j in range(len(weights)):\n",
    "        weights[j] -= learning_rate * dW[j]\n",
    "        biases[j] -= learning_rate * db[j]\n",
    "    return weights, biases, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights W1:\n",
      "[[-0.40978712 -2.32150158]\n",
      " [-0.13740566  0.52380288]]\n",
      "Final Biases b1:\n",
      "[[-0.00132608]\n",
      " [-0.0004311 ]]\n",
      "Final Weights W2:\n",
      "[[ 0.9257782   0.28017377]\n",
      " [-1.0158753  -0.56699097]]\n",
      "Final Biases b2:\n",
      "[[-0.00588794]\n",
      " [ 0.00080913]]\n",
      "Final Weights W3:\n",
      "[[-1.2820016   0.18189231]]\n",
      "Final Biases b3:\n",
      "[[0.01996305]]\n"
     ]
    }
   ],
   "source": [
    "input_size = 2\n",
    "hidden_layer_sizes = [2, 2]\n",
    "output_size = 1\n",
    "num_iterations = 2000\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = np.random.randn(input_size, 1000)\n",
    "Y = np.logical_xor(X[0, :] > 0, X[1, :] > 0).astype(int).reshape(1, -1)\n",
    "\n",
    "weights, biases, costs = backpropagation(input_size, hidden_layer_sizes, output_size,\n",
    "num_iterations, X, Y, learning_rate)\n",
    "\n",
    "for j in range(len(weights)):\n",
    "    print(f\"Final Weights W{j + 1}:\\n{weights[j]}\")\n",
    "    print(f\"Final Biases b{j + 1}:\\n{biases[j]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
